% Chapter 1

\chapter{Introduction} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 1. \emph{Introduction}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------

\section{Rise of the Web: changes in the fruition of music}
The last two decades have been highly affected by an incredibly disruptive technology: the World Wide Web. Suddenly, easy communication towards any other angle of the world was possible, making the sharing of information and content an immediate task. Many sectors have exploited this technology, for the most disparate purposes: research institutes for sharing results; global companies for providing easier access to their products, for new forms of advertisement and for collecting data about users; services for providing remotely available information. World Wide Web was soon employed also to share artistic content, particularly music: huge catalogues were made available to users (both in legal and illegal ways), providing access to ``light'' files (only a few megabytes, compared to the several tens or hundreds for movies) whose quality was comparable, if not extremely similar, to the same music piece stored on an analogic support. A company that has taken great advantage of this situation is Apple, that in 2001 launched \textit{iTunes}\footnote{\url{https://www.apple.com/itunes/}}, and audio player that was later (2003) extended into a store of digital audio files, as well as coupled by the portable device \textit{iPod}\footnote{\url{https://www.apple.com/ipod/}}. The response of the public was incredible: the service now counts almost 800 million users\footnote{\url{http://www.cnet.com/news/apple-itunes-nears-800-million-mark/}} and more than 43 million songs. Over the years, many other web-services providing access to music catalogues have been launched: \textit{Spotify}\footnote{\url{https://www.spotify.com/es/}}, \textit{Pandora}\footnote{\url{http://www.pandora.com/}} and \textit{Google Music}\footnote{\url{https://music.google.com/}} just to name the most important ones. Whether they offer a music streaming service (Spotify, Pandora) or a store of digital music (iTunes, Google Music), they are now among the most used ways of enjoying and discovering music, and the amount of music they provide access to is enormous: much more than we could ever listen to.
However, the transition to this type of services has brought to some new problems. One of them relies on the vastness of these databases: given that users want to easily discover new music suitable to their tastes through intelligently created playlists, a way to reasonably pick songs and artists among the entire catalogue is needed. \\

\section{Music Information Retrieval (MIR)}
This situation has lead to the establishment of a new interdisciplinary research field, with the purpose of providing new ways of finding information in music. This field is called \textbf{Music Information Retrieval} (MIR), and involves researchers from the fields of musicology, psychology, academic music study, signal processing and machine learning. In MIR, techniques for the extraction, management and usage of different types of data have been developed; specifically, the data involved is generally divided in \textit{metadata} and \textit{audio content descriptors}. \\
The term metadata (literally \textit{data describing data}) generally indicates all the textual data referring to a particular artist, album or track. Depending on the source and the purpose of their extraction, they might be related to very different aspects, ranging from details of a musical work (artist, year of release, genre) to data more related to users (for instance, the list of user ratings or users' listening behaviour for it). \\
On the other hand, the term audio content descriptors indicates all the data that has been extracted from the audio signal itself instead of having been mined from the Web or other popular sources. It is important to notice that there is a lack of agreement on the use of the term metadata, for sometimes audio content descriptors are also considered metadata, as they are textual information about a track. In this work we will follow the approach presented in \cite{bogdanov13}, where Bogdanov proposes to use the term metadata to indicate all the data that refers to a track and that has not been extracted from the audio signal itself.\\
Both types of data have pros and cons. Regarding metadata, major concerns arise from the questionable consistency of the descriptors among the entire catalogue catalogue of music, given that they may have been extracted from several sources. Other concerns also arise from how well they actually describe the audio track. Moreover, they generally require human intervention, which is expensive, time-consuming and prone to disagreement among different raters. On the other hand, audio content descriptors (particularly the low-level ones) may have no musical meaning and therefore they could be hard to understand. Many efforts have been taken in order to improve the methods of information extraction of both these categories. In general, however, audio content descriptors are thought to be more flexible, since they can be easily and equally computed for any track. One advantage of this technique relies on the fact that these kind of descriptors could easily be computed not just for each kind of song, but also for any segment inside of it. This has for example been exploited by \textit{Shazam}, a widely-used smartphone app for music identification that analyzes peaks in the frequency-time spectrum throughout all song length to build a very robust song identification system \cite{shazam03}. Another popular product that performs audio content analysis just for short segments of a song is The Infinite Jukebox\footnote{\url{http://infinitejuke.com}}, a web-application using \textit{The Echo Nest} library and written by Paul Lamere, that allows users to indefinitely listen to the same song, with the playback automatically jumping to points that sound very similar to the current one. The Infinite Jukebox can be considered an application of the so-called \textit{creative-MIR} \cite{xavier2013}, an emerging area of activity inner to MIR whose subject is to exploit MIR techniques for creative purposes.  Other relevant software that exploits Echo Nest library for similar purposes is Autocanonizer\footnote{\url{http://static.echonest.com/autocanonizer}} and Wub Machine\footnote{\url{http://thewubmachine.com}}. However, there aren't many commercial or research-based software tools that exploit this kind of techniques for creative interaction or manipulation of audio tracks at the moment. Probably the most relevant commercial system is Harmonic Mixing Tool\footnote{\url{http://www.idmt.fraunhofer.de/en/Service_Offerings/products_and_technologies/e_h/harmonic_mixing_tool.html}}, that performs audio content analysis on the user's music collection in order to allow a pleasant and harmonic fade when mixing between songs. More recently, the research-based software AutoMashUpper has been developed with the intent of automating generating multi-song mashup\footnote{A mashup is a composition made of two or more different songs playing together.} while also allowing the user a control over the music generated \cite{automash14}. 

%----------------------------------------------------------------------------------------

\section{Phonos Project}
Phonos project\footnote{\url{http://phonos.upf.edu/}} is an initiative of the \textbf{Music Technology Group} (Universitat Pompeu Fabra, Barcelona) in collaboration with \textbf{Phonos Foundation}. Phonos was founded in 1974 by J.M. Mestres Quadreny, Andres Lewin-Richter and Luis Callejo, and for many years it has been the only studio of electroacoustic music in Spain. Many of the electroacoustic musicians in Spain attended the courses of the composer Gabriel Brncic at Phonos and regularly organized concerts and public activities to disseminate electroacoustic music. This initiative became Phonos Foundation 1982 and in 1984 it was registered at the Generalitat de Catalunya. In 1994, an agreement of co-operation with Music Technology group was established, with the purpose of promoting cultural activities related to research in the music technology. 
In 2014, the exhibition \textit{``Phonos, 40 anys de música electrònica a Barcelona''}\footnote{\url{http://phonos.upf.edu/node/931}} has been planned at Museu de la Musica\footnote{\url{www.museumusica.bcn.es/}} (Barcelona) with the purpose of celebrating the 40th anniversary of Phonos and showing many of the instruments used in the studio, while allowing visitors to listen to the music works produced there during all these years. Given the songs' average length and their complexity, a way for the visitors to quickly and nicely explore a catalogue of songs produced in these 40 years was needed.


\begin{figure}[htbp]
  \centering
    \includegraphics[scale=0.6]{Figures/electronicmusicexhibition.jpg}
    \rule{35em}{0.5pt}
  \caption[Phonos, 40 anys de música electrònica a Barcelona]{\textit{Phonos, 40 anys de música electrònica a Barcelona}, Manifesto.}
  \label{fig:phonosmanifesto}
\end{figure}

%----------------------------------------------------------------------------------------

\section{GiantSteps}
GiantSteps\footnote{\url{http://www.giantsteps-project.eu/}} is a STREP project coordinated by JCP-Consult SAS in France in collaboration with the MTG funded by the European Commission. The aim of this project is to create the "seven-league boots" for music production in the next decade and beyond, that is, exploiting the latest fields in the field of MIR to make computer music production easier for anyone. Indeed, despite the increasing amount of software and plugins for computer music creation, it's still considered very hard to master these instruments and producing songs\footnote{ "Computer music today is like piloting a jet with all the lights turned off." (S. Jordà). \url{http://vimeo.com/28963593}} because it requires not only musical knowledge but also familiarity with the tools (both software and hardware) that the artist decide to use, and whose way of usage may greatly vary between each other. The GiantSteps project targets three different directions:
\begin{itemize}
\item Developing \textbf{musical expert agents}, that could provide suggestions from sample to song level, while guiding users lacking inspiration, technical or musical knowledge
\item Developing improved \textbf{interfaces}, implementing novel visualisation techniques that provide meaningful feedback to enable fast comprehensibility for novices and improved workflow for professionals.
\item Developing \textbf{low complexity algorithms}, so that the technologies developed can be accessible through low cost portable devices. 
\end{itemize}

Started on November 2013, GiantSteps will last 36 months and the institutions involved are:
\begin{itemize}
\item \textbf{Music Technology Group}, Universitat Pompeu Fabra, Barcelona, Spain
\item \textbf{JCP-Consult SAS}, France
\item \textbf{Johannes Kepler Universität Linz}, Austria
\item \textbf{Red Bull Music Academy}, Germany
\item \textbf{STEIM}, Amsterdam, Netherlands
\item \textbf{Reactable Systems}, Barcelona, Spain
\item \textbf{Native Instruments}, Germany
\end{itemize}

%----------------------------------------------------------------------------------------

\section{Purpose of this work}
The purpose of this work is to develop a software to be used by visitors during the exhibition \textit{Phonos, 40 anys de música electrònica a Barcelona} and that allows users to easily explore a medium-sized (few hundreds of tracks) collection of music. This software is intended to exploit latest MIR findings to create a flow of music, composed of short segments of each song, concatenated in a way that the listener can barely realize of the hops between different songs. The application must also allow users to interact with it in order to have some control over the generation of the playlist; specifically, the user should be able to give a general direction to this flow (through some sliders or others GUI elements) in regards to some relevant music features, in a way that the user-driven change in the musical output can be perceived. The application developed is meant to be part of the GiantSteps project and therefore should follow the three guidelines explained in the previous page. In addition to this, given its future use on a public place, the application is required to be easy to use also for non-musicians, as many of the visitors of the exhibition could be.

\section{Introduction to the problem of Playlist Generation}
The problem of playlist generation has already been addressed by many popular music platforms, such as \textit{Last.fm}\footnote{\url{http://last.fm}}, \textit{Pandora} and \textit{Musicovery}\footnote{\url{http://musicovery.com}}. The main objective of such services is to help users find tracks or artists that are unknown to them and that they may like, providing \textit{personalized radio playlists}. However, a playlist may be defined, in a broad way, just as a sequence of tracks \cite{bonnin14} and therefore its use could be more general. For instance, a common use of the term playlist refers to the \texit{broadcasting radio playlists}, i.e. playlists made by DJs in a radio stations and often involving popular tracks. We can therefore define the problem of playlist generation as follows \cite{bonnin14}:

\begin{displayquote}
Given (1) a pool of tracks, (2) a background knowledge database, and (3) some target characteristics of the playlist, create a sequence of tracks fulfilling the target characteristics in the best possible way.
\end{displayquote}

This task is made particularly challenging by the average size of the music database on which the generation of the playlist is needed: already, personal music libraries can be huge \cite{dias10}, hence the corresponding amount of information to be processed in order to the generate the playlist leads to very heavy computational tasks. Depending on the need of the application, these tasks may also be performed offline, although a real-time user interaction should be supported in many cases in order to allow the user to have some control over this generation process (such as in the case study of this work). As we will see in Chapter 2, extracting information from an audio signal is not a trivial task and many algorithms have considerable time-complexity, and this may lead to very long computational times already for the analysis of small-sized catalogues. Playlist generation is a well-known problem inside MIR \cite{downey03} \cite{grachten09}, since this task can be considered as a retrieval task if its definition is limited to the selection of tracks satisfying a user query \cite{bonnin14}. Other major topics of MIR also include extraction of features and similarity analysis, that can be seen as a basis for building a playlist generation system \cite{dopler08}. 

\section{Structure of the dissertation}
This dissertation is organized as follows:
\begin{itemize}
\item The first part will at first give an overview regarding music analysis techniques, explaining \textit{metadata}, audio content analysis and the differences between them. Then, common techniques of music similarity computation will be explained. Moreover, the problem of the evaluation in MIR will be introduced, with a focus on different types of evaluations generally used for automatically generated playlists. 
\item The second part will be about the methodology, explaining the different stages of the development, the problems faced and the techniques used, also in regards to the evaluation of the system. A presentation of the case study will introduce to an explanation of the reasons that lead to prefer the use of some techniques over others. 
\item Finally, results are shown and discussed, regarding both the performance of the system and the results of the evaluation conducted over a restricted number of participants. Some ideas regarding future development of the system are also presented.
\end{itemize}

